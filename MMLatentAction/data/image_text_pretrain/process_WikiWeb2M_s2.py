import copy
import os
import json
import random
import urllib.parse
import urllib.request
import hashlib
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import logging

from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("download.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# é…ç½®è·¯å¾„
BASE_DIR = Path("aburns4/WikiWeb2M")
JSON_FILES = ["pretrain-train.json", "pretrain-val.json", "pretrain-test.json"]
IMAGE_DIR = BASE_DIR / "images-86k"
TMP_DIR = Path("tmp_download")
TMP_LIST_FILE = TMP_DIR / "WikiWeb2M_image_list_AllToDownloaded.json.tmp"

# ç¡®ä¿ç›®å½•å­˜åœ¨
IMAGE_DIR.mkdir(parents=True, exist_ok=True)
TMP_DIR.mkdir(exist_ok=True)

# 1. è¯»å–æ‰€æœ‰ JSONï¼Œæå–å¹¶å»é‡ URL
def extract_urls():
    urls = set()
    for fname in JSON_FILES:
        path = BASE_DIR / fname
        if not path.exists():
            logger.warning(f"File not found: {path}")
            continue
        logger.info(f"Reading {path}...")
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        for item in data:
            sec_urls = item.get("section_image_url", [])
            for url in sec_urls:
                url = url.strip()  # å»é™¤ç©ºæ ¼ï¼ˆå¦‚ä½ ç¤ºä¾‹ä¸­çš„æœ«å°¾ç©ºæ ¼ï¼‰
                if url and url.startswith("http"):
                    urls.add(url)
    logger.info(f"Total unique URLs extracted: {len(urls)}")
    return sorted(urls)  # æ’åºä¾¿äºè°ƒè¯•/å¤ç°

# 2. ä¿å­˜æ–­ç‚¹åˆ—è¡¨
def save_url_list(urls):
    with open(TMP_LIST_FILE, "w", encoding="utf-8") as f:
        json.dump(urls, f, indent=2)
    logger.info(f"URL list saved to {TMP_LIST_FILE}")

# 3. å°† URL è½¬ä¸ºå®‰å…¨æ–‡ä»¶åï¼ˆå°½é‡ä¿ç•™å¯è¯»æ€§ï¼Œé¿å…è·¯å¾„è¿‡é•¿ï¼‰
def url_to_filename(url):
    # å‰”é™¤ scheme å’Œ domainï¼Œåªä¿ç•™ path + queryï¼ˆä½†å¤ªé•¿ä¼š hashï¼‰
    # ç­–ç•¥ï¼šç”¨ SHA256 ç”Ÿæˆå”¯ä¸€æ ‡è¯† + å‰ç¼€å¯è¯»éƒ¨åˆ†ï¼ˆæœ€å¤š 50 charsï¼‰
    parsed = urllib.parse.urlparse(url)
    base = parsed.netloc + parsed.path + parsed.query
    # å¯è¯»å‰ç¼€ï¼šå– path æœ€åéƒ¨åˆ†ï¼ˆæ–‡ä»¶åï¼‰ï¼Œæœ€å¤š 50 å­—
    basename = os.path.basename(parsed.path)
    if not basename:
        basename = "image"
    safe_basename = "".join(c if c.isalnum() or c in "._-" else "_" for c in basename)[:50]
    # ç”¨ SHA256 é˜²å†²çª
    hash_part = hashlib.sha256(url.encode()).hexdigest()[:16]
    return f"{safe_basename}_{hash_part}"

# 4. ä¸‹è½½å•ä¸ªå›¾ç‰‡ï¼ˆå¸¦é‡è¯•ï¼‰
from PIL import Image
import requests
import io
import random
import time
import logging


def to_rgb(pil_image: Image.Image, max_size: int = 560) -> Image.Image:
    # Step 1: Convert to RGB with white background for RGBA
    if pil_image.mode == 'RGBA':
        white_background = Image.new("RGB", pil_image.size, (255, 255, 255))
        white_background.paste(pil_image, mask=pil_image.split()[3])  # Use alpha channel as mask
        rgb_image = white_background
    else:
        rgb_image = pil_image.convert("RGB")

    # Step 2: Resize so that the longer side is at most `max_size`, preserving aspect ratio
    w, h = rgb_image.size
    if max(w, h) > max_size:
        # Compute new size preserving aspect ratio
        scale = max_size / max(w, h)
        new_w = int(w * scale)
        new_h = int(h * scale)
        rgb_image = rgb_image.resize((new_w, new_h), Image.Resampling.LANCZOS)  # or BICUBIC/BILINEAR

    return rgb_image


#
#
# def download_image(url, retries=3, delay=1):
#     file_name=url.replace("/","--")
#     filepath = IMAGE_DIR / file_name
#
#     print(url)
#     print(filepath)
#
#
#
#     # æ–­ç‚¹ç»­ä¼ ï¼šå·²å­˜åœ¨åˆ™è·³è¿‡
#     if filepath.exists():
#         return url, True, "already exists"
#
#     for attempt in range(1, retries + 1):
#         try:
#             with requests.get(url, stream=True) as response:
#                 response.raise_for_status()
#                 with io.BytesIO(response.content) as bio:
#                     img = copy.deepcopy(Image.open(bio))
#                 img=to_rgb(img)
#             # ä¿å­˜ä¸ºåŸæ ¼å¼ï¼ˆå¦‚ JPEG/PNGï¼‰
#             img.save(filepath, quality=95, format="JPEG")  # quality å¯¹ JPEG æœ‰æ•ˆï¼ŒPNG æ— è§†
#             return url, True, "success"
#
#         except Exception as e:
#             if filepath.exists():
#                 filepath.unlink(missing_ok=True)
#             logger.warning(f"Attempt {attempt}/{retries} failed for {url}: {type(e).__name__}: {e}")
#             if attempt < retries:
#                 time.sleep(delay * (2 ** (attempt - 1)) + random.uniform(0.5, 1.0))
#
#     return url, False, f"Failed after {retries} attempts"




import asyncio
import time
import random
import io
from pathlib import Path
from PIL import Image
import logging



# ğŸ” åŒæ­¥åŒ…è£…å™¨ï¼šè°ƒç”¨å¼‚æ­¥ playwright å‡½æ•°
def download_image(url, retries=1, delay=1):
    file_name = url.replace("/", "--").replace(":", "_")
    filepath = IMAGE_DIR / file_name

    print(f"[URL] {url}")
    print(f"[Save to] {filepath}")

    # æ–­ç‚¹ç»­ä¼ ï¼šå·²å­˜åœ¨åˆ™è·³è¿‡
    if filepath.exists():
        return url, True, "already exists"

    for attempt in range(1, retries + 1):
        try:
            # è¿è¡Œå¼‚æ­¥ä¸‹è½½é€»è¾‘
            success = asyncio.run(_download_with_playwright(url, filepath))
            if success:
                return url, True, "success"
            else:
                raise RuntimeError("Image download failed (empty or invalid response)")

        except Exception as e:
            if filepath.exists():
                filepath.unlink(missing_ok=True)
            logger.warning(f"Attempt {attempt}/{retries} failed for {url}: {type(e).__name__}: {e}")
            if attempt < retries:
                backoff = delay * (2 ** (attempt - 1)) + random.uniform(0.5, 1.0)
                time.sleep(backoff)

    return url, False, f"Failed after {retries} attempts"


async def _download_with_playwright(url: str, filepath: Path) -> bool:
    from playwright.async_api import async_playwright, Route, Request, Response
    import re

    # è§„èŒƒ URLï¼ˆå»ç©ºæ ¼ï¼‰
    url = url.strip()

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-gpu",
                "--disable-dev-shm-usage",
                "--disable-extensions",
                "--disable-plugins",
            ]
        )
        context = await browser.new_context(
            viewport={"width": 1280, "height": 720},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            # bypass_csp=True,  # æŒ‰éœ€å¼€å¯ï¼ˆæŸäº›ç«™éœ€ï¼‰
            # ignore_https_errors=True,  # æŒ‰éœ€ï¼ˆæµ‹è¯•ç¯å¢ƒï¼‰
        )
        page = await context.new_page()

        image_bytes_fut = asyncio.Future()

        # ğŸ§© æ‹¦æˆªé€»è¾‘ï¼šåŒ¹é…åŸå§‹ url æˆ–å…¶é‡å®šå‘ç›®æ ‡
        # æ³¨æ„ï¼šPlaywright route å¯¹é‡å®šå‘åçš„æœ€ç»ˆè¯·æ±‚ä¾ç„¶è§¦å‘
        async def intercept_route(route: Route):
            req: Request = route.request
            # å…³é”®ï¼šåŒ¹é…åŸå§‹ url æœ¬èº«ï¼Œæˆ–æœ€ç»ˆå“åº” URLï¼ˆåº”å¯¹é‡å®šå‘ï¼‰
            if (
                    req.url == url or
                    (hasattr(req, '_redirected_from') and req._redirected_from and req._redirected_from.url == url)
            ):
                try:
                    # ç»§ç»­è¯·æ±‚ï¼Œä½†ä¸ä¿®æ”¹
                    response = await route.fetch()
                    if response.ok and "image" in (response.headers.get("content-type") or "").lower():
                        body = await response.body()
                        if body and len(body) > 0:
                            image_bytes_fut.set_result(body)
                        else:
                            image_bytes_fut.set_exception(ValueError("Empty image body"))
                    else:
                        image_bytes_fut.set_exception(
                            ValueError(f"Non-image or non-200 response: {response.status} {response.url}")
                        )
                    await route.continue_()
                except Exception as e:
                    image_bytes_fut.set_exception(e)
                    await route.continue_()
            else:
                await route.continue_()

        # å¯ç”¨è·¯ç”±æ‹¦æˆªï¼ˆä»…é’ˆå¯¹ http/httpsï¼‰
        await context.route("**/*", intercept_route)

        try:
            # ğŸŒ å¯¼èˆªä¸€ä¸ªç©ºç™½é¡µåï¼Œæ‰‹åŠ¨è§¦å‘ GET è¯·æ±‚ï¼ˆé¿å…æ¸²æŸ“æ•´é¡µï¼‰
            # ä½¿ç”¨ page.goto + wait_until='networkidle' æœ€å¯é 
            await page.goto("https://www.google.com/", wait_until="networkidle", timeout=10000)

            # âœ… å…³é”®ï¼šç”¨ page.request.get å‘èµ·åŸç”Ÿ HTTP è¯·æ±‚ï¼ˆç»•è¿‡é¡µé¢ JS/CORSï¼‰
            # Playwright 1.22+ æ”¯æŒ context.requestï¼ˆæ›´è½»é‡ï¼‰
            # fallback to page.goto if needed
            try:
                resp = await context.request.get(
                    url,
                    headers={
                        "Referer": "https://www.google.com/",
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                    },
                    max_redirects=5,
                    timeout=15000,
                )
                if resp.ok and "image" in (resp.headers.get("content-type") or "").lower():
                    body = await resp.body()
                    if body:
                        image_bytes_fut.set_result(body)
                    else:
                        raise ValueError("Empty response body")
                else:
                    raise ValueError(f"Non-image response: {resp.status} {resp.url}")
            except Exception as e:
                # è‹¥ context.request å¤±è´¥ï¼ˆå¦‚æ—§ç‰ˆ Playwrightï¼‰ï¼Œfallback åˆ° route + navigate
                logger.debug(f"context.request failed, falling back to route + navigate: {e}")
                await page.goto(url, wait_until="networkidle", timeout=15000)

            # ç­‰å¾… image_bytes å‡†å¤‡å¥½ï¼ˆæœ€å¤š 15sï¼‰
            try:
                image_data = await asyncio.wait_for(image_bytes_fut, timeout=15.0)
            except asyncio.TimeoutError:
                raise TimeoutError("Image response not captured within timeout")

            # âœ… å®‰å…¨è§£ç ï¼šç¡®ä¿ BytesIO åœ¨ PIL ä½¿ç”¨æœŸé—´ä¸è¢« GC
            with io.BytesIO(image_data) as bio:
                # å¼ºåˆ¶åŠ è½½åˆ°å†…å­˜ï¼Œé¿å… lazy load å…³é—­é—®é¢˜
                img = Image.open(bio)
                img.load()  # ğŸ‘ˆ å…³é”®ï¼šå¼ºåˆ¶è¯»å–åƒç´ 
                img = to_rgb(img)

            # ä¿å­˜ï¼ˆæ ¹æ®æ‰©å±•åè‡ªåŠ¨é€‰æ ¼å¼ï¼‰
            suffix = filepath.suffix.lower()
            save_kwargs = {}
            if suffix in {".jpg", ".jpeg"}:
                save_kwargs.update({"format": "JPEG", "quality": 95})
            elif suffix == ".png":
                save_kwargs.update({"format": "PNG"})
            else:
                save_kwargs.update({"format": "JPEG", "quality": 95})  # fallback

            img.save(filepath, **save_kwargs)
            return True

        finally:
            await page.close()
            await context.close()
            await browser.close()






# 5. ä¸»ä¸‹è½½å‡½æ•°
def main():
    # Step 1: æå– URL åˆ—è¡¨
    if TMP_LIST_FILE.exists():
        logger.info(f"Resuming from existing list: {TMP_LIST_FILE}")
        with open(TMP_LIST_FILE, 'r', encoding='utf-8') as f:
            urls = json.load(f)
        remain_urls=[]
        for url in urls:
            file_name = url.replace("/", "--").replace(":", "_")
            filepath = IMAGE_DIR / file_name

            if not filepath.exists():
                remain_urls.append(url)

        with open(TMP_LIST_FILE, "w", encoding="utf-8") as f:
            json.dump(remain_urls, f, indent=2)

        logger.info(f"Remain: {len(remain_urls)}")

        urls = remain_urls

    else:
        urls = extract_urls()
        save_url_list(urls)

    # Step 2: è¿‡æ»¤å·²ä¸‹è½½ï¼ˆæŒ‰æ–‡ä»¶å­˜åœ¨åˆ¤æ–­ï¼‰
    to_download = urls


    # Step 3: å¤šçº¿ç¨‹ä¸‹è½½
    success, fail = 0, 0
    with ThreadPoolExecutor(max_workers=16) as executor:  # å¯è°ƒ
        future_to_url = {executor.submit(download_image, url): url for url in to_download}
        for i, future in enumerate(tqdm(as_completed(future_to_url), total=len(to_download), desc="Downloading", unit="img"), 1):
            url, ok, msg = future.result()
            if ok:
                success += 1
                logger.info(f"[{i}/{len(to_download)}] âœ… {url} â†’ {msg}")
            else:
                fail += 1
                logger.error(f"[{i}/{len(to_download)}] âŒ {url} â†’ {msg}")

    logger.info(f"âœ… Done. Success: {success}, Fail: {fail} / Total Requested: {len(to_download)}")

if __name__ == "__main__":
    main()